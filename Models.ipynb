{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras - neural networks framework\n",
    "import keras\n",
    "from keras import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "# pandas and numpy to format the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# scikitlearn to split and normalize data\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# other packages\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to fix data types using apply from pandas\n",
    "\n",
    "def only_nbr(x):\n",
    "    return float(re.sub(\"[^0-9\\.]\", \"\", str(x)))\n",
    "\n",
    "def to_float(x):\n",
    "    return float(re.sub(\"\\,\", \".\", str(x)))\n",
    "\n",
    "def to_bin(x):\n",
    "    if(x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "\n",
    "x = pd.read_csv('dataset_X_y.csv') #mudar aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,38):\n",
    "    x.iloc[:,i] = x.iloc[:,i].apply(to_bin)\n",
    "\n",
    "# dropping non-numeric and unusable columns\n",
    "x = x.drop(columns=['ano_censo', 'cod_escola', 'cod_municipio', 'municipio', 'regiao', 'unidade_federativa', 'Rede'])\n",
    "x['NotaProvaBrasil_MT_current_year'] = x['NotaProvaBrasil_MT_current_year'].apply(only_nbr)\n",
    "x['NotaProvaBrasil_LP_current_year'] = x['NotaProvaBrasil_LP_current_year'].apply(only_nbr)\n",
    "\n",
    "# creating ground truth\n",
    "y = x['Ideb_next_2_years']\n",
    "\n",
    "# excluding from inputs\n",
    "x = x.drop(columns=['Ideb_next_2_years'])\n",
    "input_dim = len(x.columns)\n",
    "x_df = x\n",
    "# normalizing data 0-1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "# split train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "def create_model1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(adam, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def create_model2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(adam, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.5121 - val_loss: 0.2959\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 21us/step - loss: 0.2824 - val_loss: 0.3075\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2816 - val_loss: 0.2865\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2807 - val_loss: 0.2871\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 21us/step - loss: 0.2780 - val_loss: 0.2898\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 21us/step - loss: 0.2785 - val_loss: 0.2849\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2776 - val_loss: 0.2813\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2754 - val_loss: 0.2784\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2757 - val_loss: 0.2784\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2745 - val_loss: 0.2844\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2740 - val_loss: 0.2954\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2735 - val_loss: 0.2814\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2727 - val_loss: 0.2779\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2720 - val_loss: 0.2815\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2709 - val_loss: 0.2783\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2717 - val_loss: 0.2765\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2710 - val_loss: 0.2775\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2714 - val_loss: 0.3086\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2700 - val_loss: 0.2791\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2698 - val_loss: 0.2760\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2683 - val_loss: 0.2778\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2691 - val_loss: 0.2827\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2677 - val_loss: 0.2816\n",
      "Epoch 24/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2681 - val_loss: 0.3039\n",
      "Epoch 25/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2686 - val_loss: 0.2866\n",
      "Epoch 26/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2668 - val_loss: 0.2872\n",
      "Epoch 27/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2670 - val_loss: 0.2869\n",
      "Epoch 28/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2662 - val_loss: 0.2892\n",
      "Epoch 29/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2655 - val_loss: 0.2819\n",
      "Epoch 30/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2648 - val_loss: 0.2822\n",
      "20510/20510 [==============================] - 0s 8us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.5193 - val_loss: 0.2838\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2874 - val_loss: 0.2838\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2820 - val_loss: 0.3192\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2794 - val_loss: 0.3537\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2800 - val_loss: 0.2862\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2786 - val_loss: 0.2838\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2775 - val_loss: 0.2800\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2768 - val_loss: 0.2804\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2760 - val_loss: 0.2779\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2747 - val_loss: 0.2860\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2753 - val_loss: 0.2779\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2737 - val_loss: 0.2793\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2727 - val_loss: 0.2778\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2718 - val_loss: 0.2880\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2720 - val_loss: 0.2788\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 2s 34us/step - loss: 0.2716 - val_loss: 0.3061\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2716 - val_loss: 0.2808\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2702 - val_loss: 0.2918\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2695 - val_loss: 0.2816\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2695 - val_loss: 0.3118\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2691 - val_loss: 0.3114\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2683 - val_loss: 0.2794\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2683 - val_loss: 0.2797\n",
      "20510/20510 [==============================] - 0s 9us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.8499 - val_loss: 0.3004\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2808 - val_loss: 0.2804\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2795 - val_loss: 0.2864\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2789 - val_loss: 0.2786\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2773 - val_loss: 0.2799\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2768 - val_loss: 0.2764\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2756 - val_loss: 0.2955\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2755 - val_loss: 0.2882\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2752 - val_loss: 0.2771\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2743 - val_loss: 0.2845\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2738 - val_loss: 0.2775\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2727 - val_loss: 0.2857\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2740 - val_loss: 0.2917\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2722 - val_loss: 0.2760\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2720 - val_loss: 0.2829\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2719 - val_loss: 0.2734\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2712 - val_loss: 0.2769\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2711 - val_loss: 0.2756\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2699 - val_loss: 0.2780\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2705 - val_loss: 0.2798\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2696 - val_loss: 0.2747\n",
      "Epoch 22/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2701 - val_loss: 0.2731\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2693 - val_loss: 0.2748\n",
      "Epoch 24/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2695 - val_loss: 0.2789\n",
      "Epoch 25/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2693 - val_loss: 0.2771\n",
      "Epoch 26/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2685 - val_loss: 0.2755\n",
      "Epoch 27/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2682 - val_loss: 0.2749\n",
      "Epoch 28/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2673 - val_loss: 0.2774\n",
      "Epoch 29/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2671 - val_loss: 0.2735\n",
      "Epoch 30/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2673 - val_loss: 0.2786\n",
      "Epoch 31/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2671 - val_loss: 0.2746\n",
      "Epoch 32/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2676 - val_loss: 0.2993\n",
      "20510/20510 [==============================] - 0s 7us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.4696 - val_loss: 0.2915\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 21us/step - loss: 0.2851 - val_loss: 0.2847\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2852 - val_loss: 0.2790\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2823 - val_loss: 0.2831\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2818 - val_loss: 0.2801\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2792 - val_loss: 0.2832\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 20us/step - loss: 0.2775 - val_loss: 0.2768\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2775 - val_loss: 0.2931\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2763 - val_loss: 0.2812\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 21us/step - loss: 0.2754 - val_loss: 0.2790\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2754 - val_loss: 0.2757\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2742 - val_loss: 0.3017\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2735 - val_loss: 0.2749\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 20us/step - loss: 0.2718 - val_loss: 0.2775\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 21us/step - loss: 0.2711 - val_loss: 0.2777\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2714 - val_loss: 0.2738\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2705 - val_loss: 0.2753\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2700 - val_loss: 0.2752\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2695 - val_loss: 0.2823\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 30us/step - loss: 0.2692 - val_loss: 0.2745\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2679 - val_loss: 0.2741\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2687 - val_loss: 0.2758\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2681 - val_loss: 0.2726\n",
      "Epoch 24/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2663 - val_loss: 0.3130\n",
      "Epoch 25/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2671 - val_loss: 0.2923\n",
      "Epoch 26/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2658 - val_loss: 0.2731\n",
      "Epoch 27/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2652 - val_loss: 0.2737\n",
      "Epoch 28/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2639 - val_loss: 0.2729\n",
      "Epoch 29/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2642 - val_loss: 0.2788\n",
      "Epoch 30/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2636 - val_loss: 0.2756\n",
      "Epoch 31/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2638 - val_loss: 0.2793\n",
      "Epoch 32/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2630 - val_loss: 0.2736\n",
      "Epoch 33/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2625 - val_loss: 0.2730\n",
      "20510/20510 [==============================] - 0s 9us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 1s 30us/step - loss: 0.4877 - val_loss: 0.2986\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2842 - val_loss: 0.2817\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2818 - val_loss: 0.2789\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2800 - val_loss: 0.3260\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2799 - val_loss: 0.2882\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2768 - val_loss: 0.2874\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2768 - val_loss: 0.2960\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2762 - val_loss: 0.3048\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2746 - val_loss: 0.3315\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2747 - val_loss: 0.2927\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2735 - val_loss: 0.2793\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2726 - val_loss: 0.2893\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2722 - val_loss: 0.2811\n",
      "20510/20510 [==============================] - 0s 10us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 2s 33us/step - loss: 0.7231 - val_loss: 0.2814\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2824 - val_loss: 0.2877\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2808 - val_loss: 0.2982\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 21us/step - loss: 0.2799 - val_loss: 0.3068\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2789 - val_loss: 0.2774\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2768 - val_loss: 0.2799\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2761 - val_loss: 0.2786\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2763 - val_loss: 0.2855\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2741 - val_loss: 0.2816\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2734 - val_loss: 0.2755\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2737 - val_loss: 0.3035\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2726 - val_loss: 0.2743\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2724 - val_loss: 0.2754\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2713 - val_loss: 0.2766\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 2s 33us/step - loss: 0.2707 - val_loss: 0.2774\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 32us/step - loss: 0.2700 - val_loss: 0.2886\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2694 - val_loss: 0.2745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2684 - val_loss: 0.3106\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2678 - val_loss: 0.2870\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2678 - val_loss: 0.2844\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2669 - val_loss: 0.2747\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2669 - val_loss: 0.2741\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2664 - val_loss: 0.2778\n",
      "Epoch 24/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2658 - val_loss: 0.2888\n",
      "Epoch 25/200\n",
      "46146/46146 [==============================] - 1s 22us/step - loss: 0.2656 - val_loss: 0.2755\n",
      "Epoch 26/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2650 - val_loss: 0.3368\n",
      "Epoch 27/200\n",
      "46146/46146 [==============================] - 1s 23us/step - loss: 0.2642 - val_loss: 0.2765\n",
      "Epoch 28/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2640 - val_loss: 0.2746\n",
      "Epoch 29/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2631 - val_loss: 0.2779\n",
      "Epoch 30/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2628 - val_loss: 0.2774\n",
      "Epoch 31/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2632 - val_loss: 0.2830\n",
      "Epoch 32/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2627 - val_loss: 0.2762\n",
      "20510/20510 [==============================] - 0s 11us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.4954 - val_loss: 0.2854\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2822 - val_loss: 0.2891\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2822 - val_loss: 0.3232\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 24us/step - loss: 0.2805 - val_loss: 0.2783\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2781 - val_loss: 0.2772\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2770 - val_loss: 0.2755\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2769 - val_loss: 0.2791\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2748 - val_loss: 0.2772\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2740 - val_loss: 0.2787\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2730 - val_loss: 0.2902\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2734 - val_loss: 0.2739\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2730 - val_loss: 0.2835\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2718 - val_loss: 0.2735\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2717 - val_loss: 0.2880\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2704 - val_loss: 0.2765\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2700 - val_loss: 0.2782\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 2s 36us/step - loss: 0.2696 - val_loss: 0.2768\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 30us/step - loss: 0.2690 - val_loss: 0.2902\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - ETA: 0s - loss: 0.267 - 2s 34us/step - loss: 0.2679 - val_loss: 0.2950\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 2s 34us/step - loss: 0.2685 - val_loss: 0.2768\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2670 - val_loss: 0.2813\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2669 - val_loss: 0.2760\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2663 - val_loss: 0.2744\n",
      "20510/20510 [==============================] - 0s 10us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 2s 35us/step - loss: 0.5393 - val_loss: 0.2964\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2841 - val_loss: 0.2844\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2822 - val_loss: 0.2847\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2796 - val_loss: 0.2767\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2794 - val_loss: 0.2747\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2779 - val_loss: 0.2882\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2770 - val_loss: 0.2883\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2759 - val_loss: 0.2929\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2754 - val_loss: 0.2790\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2747 - val_loss: 0.2745\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2743 - val_loss: 0.2827\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2727 - val_loss: 0.2819\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2712 - val_loss: 0.2758\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2734 - val_loss: 0.2814\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2704 - val_loss: 0.2759\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2700 - val_loss: 0.2898\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2691 - val_loss: 0.2858\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2682 - val_loss: 0.2753\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2686 - val_loss: 0.2736\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2669 - val_loss: 0.2766\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2671 - val_loss: 0.2795\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2672 - val_loss: 0.2866\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 31us/step - loss: 0.2664 - val_loss: 0.2888\n",
      "Epoch 24/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2666 - val_loss: 0.2756\n",
      "Epoch 25/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2654 - val_loss: 0.2750\n",
      "Epoch 26/200\n",
      "46146/46146 [==============================] - 1s 32us/step - loss: 0.2641 - val_loss: 0.2811\n",
      "Epoch 27/200\n",
      "46146/46146 [==============================] - 1s 30us/step - loss: 0.2644 - val_loss: 0.2747\n",
      "Epoch 28/200\n",
      "46146/46146 [==============================] - 1s 30us/step - loss: 0.2643 - val_loss: 0.2769\n",
      "Epoch 29/200\n",
      "46146/46146 [==============================] - 2s 34us/step - loss: 0.2632 - val_loss: 0.2746\n",
      "20510/20510 [==============================] - 0s 12us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 2s 33us/step - loss: 0.4816 - val_loss: 0.3253\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2828 - val_loss: 0.2872\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2810 - val_loss: 0.2799\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2796 - val_loss: 0.2814\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2793 - val_loss: 0.2801\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2780 - val_loss: 0.2780\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2760 - val_loss: 0.2871\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2775 - val_loss: 0.2884\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2744 - val_loss: 0.2830\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2745 - val_loss: 0.2763\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2729 - val_loss: 0.2775\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2739 - val_loss: 0.2987\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2709 - val_loss: 0.2894\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2706 - val_loss: 0.2795\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2702 - val_loss: 0.2807\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2699 - val_loss: 0.2860\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2688 - val_loss: 0.2794\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2691 - val_loss: 0.2786\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2687 - val_loss: 0.2761\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2672 - val_loss: 0.2770\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2670 - val_loss: 0.2763\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2665 - val_loss: 0.2762\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2679 - val_loss: 0.2790\n",
      "Epoch 24/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2661 - val_loss: 0.2770\n",
      "Epoch 25/200\n",
      "46146/46146 [==============================] - 1s 30us/step - loss: 0.2647 - val_loss: 0.2823\n",
      "Epoch 26/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2651 - val_loss: 0.2779\n",
      "Epoch 27/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2657 - val_loss: 0.2764\n",
      "Epoch 28/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2647 - val_loss: 0.2842\n",
      "Epoch 29/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2643 - val_loss: 0.2760\n",
      "Epoch 30/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2639 - val_loss: 0.2777\n",
      "Epoch 31/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2626 - val_loss: 0.2751\n",
      "Epoch 32/200\n",
      "46146/46146 [==============================] - 1s 25us/step - loss: 0.2625 - val_loss: 0.2769\n",
      "Epoch 33/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2629 - val_loss: 0.2780\n",
      "Epoch 34/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2623 - val_loss: 0.2832\n",
      "Epoch 35/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2625 - val_loss: 0.2935\n",
      "Epoch 36/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2620 - val_loss: 0.2772\n",
      "Epoch 37/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2614 - val_loss: 0.2864\n",
      "Epoch 38/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2608 - val_loss: 0.2834\n",
      "Epoch 39/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2614 - val_loss: 0.2776\n",
      "Epoch 40/200\n",
      "46146/46146 [==============================] - 1s 27us/step - loss: 0.2602 - val_loss: 0.2851\n",
      "Epoch 41/200\n",
      "46146/46146 [==============================] - 1s 26us/step - loss: 0.2603 - val_loss: 0.2807\n",
      "20510/20510 [==============================] - 0s 11us/step\n",
      "Train on 46146 samples, validate on 15383 samples\n",
      "Epoch 1/200\n",
      "46146/46146 [==============================] - 2s 36us/step - loss: 0.4966 - val_loss: 0.2826\n",
      "Epoch 2/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2835 - val_loss: 0.2819\n",
      "Epoch 3/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2819 - val_loss: 0.2837\n",
      "Epoch 4/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2798 - val_loss: 0.2842\n",
      "Epoch 5/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2787 - val_loss: 0.2855\n",
      "Epoch 6/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2784 - val_loss: 0.2777\n",
      "Epoch 7/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2771 - val_loss: 0.3215\n",
      "Epoch 8/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2768 - val_loss: 0.2822\n",
      "Epoch 9/200\n",
      "46146/46146 [==============================] - 2s 34us/step - loss: 0.2761 - val_loss: 0.2765\n",
      "Epoch 10/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2745 - val_loss: 0.2783\n",
      "Epoch 11/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2731 - val_loss: 0.2862\n",
      "Epoch 12/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2732 - val_loss: 0.2908\n",
      "Epoch 13/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2722 - val_loss: 0.3255\n",
      "Epoch 14/200\n",
      "46146/46146 [==============================] - 1s 28us/step - loss: 0.2735 - val_loss: 0.2757\n",
      "Epoch 15/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2713 - val_loss: 0.2904\n",
      "Epoch 16/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2703 - val_loss: 0.2764\n",
      "Epoch 17/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2704 - val_loss: 0.2888\n",
      "Epoch 18/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2702 - val_loss: 0.2803\n",
      "Epoch 19/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2690 - val_loss: 0.2878\n",
      "Epoch 20/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2689 - val_loss: 0.2793\n",
      "Epoch 21/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2674 - val_loss: 0.2793\n",
      "Epoch 22/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2678 - val_loss: 0.2767\n",
      "Epoch 23/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2668 - val_loss: 0.2777\n",
      "Epoch 24/200\n",
      "46146/46146 [==============================] - 1s 29us/step - loss: 0.2665 - val_loss: 0.2783\n",
      "20510/20510 [==============================] - 0s 12us/step\n"
     ]
    }
   ],
   "source": [
    "# it's not k-fold cross validation, but helps to understand how is the loss in more than one case\n",
    "\n",
    "k = 5\n",
    "loss1 = []\n",
    "loss2 = []\n",
    "for i in range(1,k+1):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, random_state=i)\n",
    "    \n",
    "    model1 = create_model1()\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience = 10)\n",
    "    model1.fit(x_train, y_train, batch_size=32, epochs=200, callbacks=[es], validation_split=0.25)\n",
    "    loss1.append(model1.evaluate(x_test, y_test))\n",
    "    \n",
    "    model2 = create_model2()\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience = 10)\n",
    "    model2.fit(x_train, y_train, batch_size=32, epochs=200, callbacks=[es], validation_split=0.25)\n",
    "    loss2.append(model2.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28229642398615457\n",
      "0.2743664767153957\n"
     ]
    }
   ],
   "source": [
    "# mean MSE\n",
    "print(np.mean(loss1))\n",
    "print(np.mean(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing growth and decay to create a confusion matrix\n",
    "\n",
    "new_scaler = MinMaxScaler()\n",
    "new_scaler.fit([[scaler.data_min_[-3]], [scaler.data_max_[-3]]])\n",
    "\n",
    "model1_n = new_scaler.transform(model1.predict(x_test)[:,:])\n",
    "model2_n = new_scaler.transform(model2.predict(x_test)[:,:])\n",
    "y_test_n = new_scaler.transform([np.array(y_test)])\n",
    "y_test_n = np.transpose(y_test_n)\n",
    "x_test_n = [x_test[:,-3]]\n",
    "x_test_n = np.transpose(x_test_n)\n",
    "\n",
    "# predito - atual\n",
    "d1 = model1_n - x_test_n\n",
    "d2 = model2_n - x_test_n\n",
    "\n",
    "# real - atual\n",
    "d3 = y_test_n - x_test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating confusion matrix\n",
    "\n",
    "kind1 = []\n",
    "kind2 = []\n",
    "a = -1\n",
    "b = -1\n",
    "for i, j, k in zip(d1,d2,d3):\n",
    "    if(i >= 0 and k > 0): ## true positive\n",
    "        a = 1\n",
    "    elif(i >= 0 and k < 0): ## false positive\n",
    "        a = 2\n",
    "    elif(i < 0 and k > 0): ## false negative\n",
    "        a = 3\n",
    "    else: ## true negative\n",
    "        a = 4\n",
    "    \n",
    "    if(j >= 0 and k > 0): ## true positive\n",
    "        b = 1\n",
    "    elif(j > 0 and k < 0): ## false positive\n",
    "        b = 2\n",
    "    elif(j < 0 and k > 0): ## false negative\n",
    "        b = 3\n",
    "    else: ## true negative\n",
    "        b = 4\n",
    "    kind1.append(a)\n",
    "    kind2.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9953, 4281, 1896, 4380]\n",
      "[10050, 4343, 1799, 4318]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = [kind1.count(1), kind1.count(2), kind1.count(3), kind1.count(4)]\n",
    "print(confusion_matrix)\n",
    "confusion_matrix = [kind2.count(1), kind2.count(2), kind2.count(3), kind2.count(4)]\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model2.layers[0].get_weights()[0].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pib\n",
      "-5.068445\n",
      "----------------------------\n",
      "pop\n",
      "-2.6512885\n",
      "----------------------------\n",
      "num_estudantes_ensino_fund_anos_finais\n",
      "-5.024829\n",
      "----------------------------\n",
      "num_professores_em_regencia_fund_af\n",
      "-1.0396985\n",
      "----------------------------\n",
      "in_agua_filtrada\n",
      "-2.6972737\n",
      "----------------------------\n",
      "in_agua_inexistente\n",
      "0.92397934\n",
      "----------------------------\n",
      "in_energia_rede_publica\n",
      "-1.2199844\n",
      "----------------------------\n",
      "in_energia_inexistente\n",
      "-1.4226849\n",
      "----------------------------\n",
      "in_esgoto_rede_publica\n",
      "-1.4637947\n",
      "----------------------------\n",
      "in_esgoto_inexistente\n",
      "-0.5608607\n",
      "----------------------------\n",
      "in_sala_professor\n",
      "-2.369531\n",
      "----------------------------\n",
      "in_laboratorio_informatica\n",
      "-2.3656266\n",
      "----------------------------\n",
      "in_laboratorio_ciencias\n",
      "-1.0382903\n",
      "----------------------------\n",
      "in_sala_atendimento_especial\n",
      "-1.0101061\n",
      "----------------------------\n",
      "in_quadra_esportes\n",
      "0.14796466\n",
      "----------------------------\n",
      "in_cozinha\n",
      "-1.3042468\n",
      "----------------------------\n",
      "in_biblioteca\n",
      "-1.7222735\n",
      "----------------------------\n",
      "in_sala_leitura\n",
      "-5.856965\n",
      "----------------------------\n",
      "in_biblioteca_sala_leitura\n",
      "-2.507801\n",
      "----------------------------\n",
      "in_parque_infantil\n",
      "-1.897365\n",
      "----------------------------\n",
      "in_bercario\n",
      "4.191422\n",
      "----------------------------\n",
      "in_banheiro_fora_predio\n",
      "-1.0464829\n",
      "----------------------------\n",
      "in_banheiro_dentro_predio\n",
      "-2.1490104\n",
      "----------------------------\n",
      "in_secretaria\n",
      "-0.9454729\n",
      "----------------------------\n",
      "in_refeitorio\n",
      "-2.1207392\n",
      "----------------------------\n",
      "in_auditorio\n",
      "-0.4916482\n",
      "----------------------------\n",
      "in_patio_coberto\n",
      "-1.8640397\n",
      "----------------------------\n",
      "in_patio_descoberto\n",
      "-0.4865427\n",
      "----------------------------\n",
      "in_alojam_aluno\n",
      "-1.303458\n",
      "----------------------------\n",
      "in_alojam_professor\n",
      "-2.2283924\n",
      "----------------------------\n",
      "in_area_verde\n",
      "-0.5946982\n",
      "----------------------------\n",
      "in_lavanderia\n",
      "-0.72278005\n",
      "----------------------------\n",
      "num_salas_utilizadas\n",
      "-0.2195816\n",
      "----------------------------\n",
      "num_equip_tv\n",
      "-0.20038944\n",
      "----------------------------\n",
      "num_equip_videocassete\n",
      "0.106074214\n",
      "----------------------------\n",
      "num_equip_dvd\n",
      "2.2718248\n",
      "----------------------------\n",
      "num_equip_parabolica\n",
      "4.74115\n",
      "----------------------------\n",
      "num_equip_copiadora\n",
      "0.781493\n",
      "----------------------------\n",
      "num_equip_retroprojetor\n",
      "-0.6248677\n",
      "----------------------------\n",
      "num_equip_impressora\n",
      "3.7402258\n",
      "----------------------------\n",
      "num_equip_impressora_mult\n",
      "-1.0404092\n",
      "----------------------------\n",
      "num_equip_som\n",
      "9.46414\n",
      "----------------------------\n",
      "num_equip_multimidia\n",
      "6.271324\n",
      "----------------------------\n",
      "num_equip_fax\n",
      "-0.82935107\n",
      "----------------------------\n",
      "num_equip_foto\n",
      "3.5750308\n",
      "----------------------------\n",
      "num_computador\n",
      "0.76350385\n",
      "----------------------------\n",
      "num_comp_administrativo\n",
      "7.9406824\n",
      "----------------------------\n",
      "num_comp_aluno\n",
      "5.3085246\n",
      "----------------------------\n",
      "in_internet\n",
      "-0.42054707\n",
      "----------------------------\n",
      "num_funcionarios\n",
      "-6.9851027\n",
      "----------------------------\n",
      "aluno_prof_ratio\n",
      "-1.7035346\n",
      "----------------------------\n",
      "NotaProvaBrasil_MT_current_year\n",
      "0.39634997\n",
      "----------------------------\n",
      "NotaProvaBrasil_LP_current_year\n",
      "-2.728888\n",
      "----------------------------\n",
      "NotaProvaBrasil_NotaMedia_current_year\n",
      "4.5313487\n",
      "----------------------------\n",
      "IndicadorRendimento__currrent_year\n",
      "1.1997815\n",
      "----------------------------\n",
      "Ideb_current_year\n",
      "3.49441\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# verifying weights of the first layer\n",
    "\n",
    "dict = {}\n",
    "for i in range(56):\n",
    "    dict[x_df.columns[i]] = w[i]\n",
    "\n",
    "for i in dict:\n",
    "    print(i)\n",
    "    print(dict[i])\n",
    "    print('----------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
